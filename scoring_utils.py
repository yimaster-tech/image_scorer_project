# scoring_utils.py
import torch
import cv2
import os
import numpy as np
from PIL import Image
import clip
import pandas as pd
from tqdm import tqdm
from insightface.app import FaceAnalysis
from concurrent.futures import ThreadPoolExecutor
from transformers import CLIPProcessor, CLIPModel
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Tuple, Dict
# è®¾å¤‡é…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
from Config import Config
SIM_THRESHOLD = Config.SIM_THRESHOLD

# åˆå§‹åŒ–æ¨¡å‹
# æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
local_model_path = "./models/clip-vit-large-patch14"
clip_model = CLIPModel.from_pretrained(local_model_path).to(DEVICE)
clip_processor = CLIPProcessor.from_pretrained(local_model_path)

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆç»Ÿä¸€ç®¡ç†é¿å…é‡å¤åˆå§‹åŒ–ï¼‰
clip_model_aes, preprocess_aes = clip.load("./models/ViT-L-14.pt", device=DEVICE)  # CLIP æ¨¡å‹
aesthetic_model = torch.nn.Linear(768, 1)  # ç¾å­¦è¯„åˆ†æ¨¡å‹
aesthetic_model.load_state_dict(torch.load("./models/sa_0_4_vit_l_14_linear.pth", map_location=DEVICE))
aesthetic_model.to(DEVICE)
aesthetic_model.eval()
face_app = FaceAnalysis(name="buffalo_l")  # äººè„¸åˆ†ææ¨¡å‹
face_app.prepare(ctx_id=0 if torch.cuda.is_available() else -1)

def load_image(path: str) -> np.ndarray:
    # """å®‰å…¨åŠ è½½å›¾åƒå¹¶é‡Šæ”¾èµ„æº"""
    try:
        with open(path, 'rb') as f:
            img = cv2.imdecode(np.frombuffer(f.read(), dtype=np.uint8), cv2.IMREAD_COLOR)
        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB) if img is not None else None
    except Exception as e:
        print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥ {path}: {e}")
        return None
def get_image_map(image_folder: str) -> dict:
    """åŠ è½½å›¾åƒç›®å½•ä¸º {æ–‡ä»¶å: å›¾åƒæ•°ç»„(BGR)} å­—å…¸"""
    images = {}
    for filename in os.listdir(image_folder):
        if filename.lower().endswith((".jpg", ".png", ".jpeg", ".webp")):
            img_path = os.path.join(image_folder, filename)
            img = load_image(img_path)
            if img is not None:
                images[filename] = img
    return images

@torch.no_grad()
def get_aesthetic_score(image_path: str) -> float:
    """è®¡ç®—å•å¼ å›¾åƒçš„ç¾å­¦è¯„åˆ†"""
    try:
        image = preprocess_aes(Image.open(image_path).convert("RGB")).unsqueeze(0).to(DEVICE)
        image_features = clip_model_aes.encode_image(image).float()
        image_features /= image_features.norm(dim=-1, keepdim=True)
        return aesthetic_model(image_features).item()
    except Exception as e:
        print(f"âŒ ç¾å­¦è¯„åˆ†å¤±è´¥ {image_path}: {e}")
        return -1.0

def compute_blur_score(image_path: str) -> float:
    """è®¡ç®—å•å¼ å›¾åƒçš„æ¸…æ™°åº¦è¯„åˆ†ï¼ˆæ‹‰æ™®æ‹‰æ–¯æ–¹å·®ï¼‰"""
    img_cv = cv2.imread(image_path)
    return cv2.Laplacian(img_cv, cv2.CV_64F).var() if img_cv is not None else -1.0

def face_similarity_tools(image1: np.ndarray, image2: np.ndarray) -> float:
    """è®¡ç®—ä¸¤å¼ å›¾åƒçš„äººè„¸æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆæ”¯æŒå¤šäººè„¸ï¼‰"""
    faces1 = face_app.get(image1)
    faces2 = face_app.get(image2)
    if not faces1 or not faces2:
        raise ValueError("æ£€æµ‹åˆ°æ— è„¸å›¾åƒï¼Œç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥")
    
    max_sim = -1.0
    for f1 in faces1:
        for f2 in faces2:
            if hasattr(f1, 'pose') and hasattr(f2, 'pose'):
                yaw_diff = abs(f1.pose[0] - f2.pose[0])
                pitch_diff = abs(f1.pose[1] - f2.pose[1])
                roll_diff = abs(f1.pose[2] - f2.pose[2])
                if yaw_diff > 20 or pitch_diff > 20 or roll_diff > 20:
                    print(f"âš ï¸ å§¿æ€è§’åº¦å·®å¼‚è¾ƒå¤§ï¼šyaw={yaw_diff:.1f}, pitch={pitch_diff:.1f}, roll={roll_diff:.1f}")

            emb1, emb2 = f1.embedding, f2.embedding
            sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            max_sim = max(max_sim, sim)
    return round(max_sim * 100, 2)

def batch_face_similarity(image_map: dict) -> tuple[list, float]:
    """æ‰¹é‡è®¡ç®—å›¾åƒä¸¤ä¸¤é—´çš„äººè„¸ç›¸ä¼¼åº¦"""
    total_sim, count = 0.0, 0
    results = []
    image_list = list(image_map.items())

    for i in range(len(image_list)):
        for j in range(i + 1, len(image_list)):
            (name1, img1), (name2, img2) = image_list[i], image_list[j]
            try:
                sim = face_similarity_tools(img1, img2)
                print(f"{name1} <--> {name2} ç›¸ä¼¼åº¦: {sim}")
                results.append((name1, name2, sim))
                total_sim += sim
                count += 1
            except Exception as e:
                print(f"âŒ äººè„¸ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ {name1}-{name2}: {e}")

    avg_sim = round(total_sim / count, 2) if count > 0 else 0.0
    print(f"å¹³å‡ç›¸ä¼¼åº¦: {avg_sim}")
    return results, avg_sim

def extract_clip_features(image_paths: List[str]) -> Tuple[List[np.ndarray], List[str]]:
    """æå–å›¾åƒçš„ CLIP ç‰¹å¾"""
    features = []
    valid_paths = []
    print("ğŸ” æ­£åœ¨æå–å›¾åƒç‰¹å¾...")
    for path in tqdm(image_paths):
        try:
            image = Image.open(path).convert("RGB")
            inputs = clip_processor(images=image, return_tensors="pt").to(DEVICE)
            with torch.no_grad():
                feat = clip_model.get_image_features(**inputs)
                feat = feat / feat.norm(p=2, dim=-1, keepdim=True)  # å½’ä¸€åŒ–
                features.append(feat.cpu().numpy()[0])
                valid_paths.append(path)
        except Exception as e:
            print(f"âŒ è·³è¿‡ {path}ï¼ŒåŸå› ï¼š{e}")
    return features, valid_paths

def compute_similarity_and_report(features: np.ndarray, paths: List[str], threshold: float) -> List[Tuple[str, str, float]]:
    """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µå¹¶è¾“å‡ºé«˜äºé˜ˆå€¼çš„å›¾åƒå¯¹"""
    print("ğŸ“ æ­£åœ¨è®¡ç®—å›¾åƒå¯¹ç›¸ä¼¼åº¦...")
    sim_matrix = cosine_similarity(features)
    results = []
    for i in range(len(paths)):
        for j in range(i + 1, len(paths)):
            sim = sim_matrix[i][j]
            if sim > threshold:
                img1 = os.path.basename(paths[i])
                img2 = os.path.basename(paths[j])
                print(f"{img1} <--> {img2} | ç›¸ä¼¼åº¦: {sim:.4f}")
                results.append((img1, img2, sim))
    if not results:
        print("âœ… æ²¡æœ‰å‘ç°é«˜ç›¸ä¼¼åº¦å›¾åƒå¯¹ï¼Œæ•°æ®é›†è¾ƒä¸ºå¤šæ ·ã€‚")
    return results

def batch_clip_similarity(image_dir: str, image_map: Dict[str, np.ndarray]) -> List[Tuple[str, str, float]]:
    """
    æ‰¹é‡è®¡ç®—å›¾åƒä¹‹é—´çš„ CLIP ç›¸ä¼¼åº¦ï¼Œå¹¶è¾“å‡ºé«˜äºé˜ˆå€¼çš„å›¾åƒå¯¹ã€‚
    è¿”å›å€¼: [(image1, image2, similarity_score), ...]
    """
    image_paths = [os.path.join(image_dir, filename) for filename in image_map.keys()]
    features, valid_paths = extract_clip_features(image_paths)
    if not features:
        print("âš ï¸ æ— æœ‰æ•ˆå›¾åƒç‰¹å¾ï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—ã€‚")
        return []
    return compute_similarity_and_report(np.stack(features), valid_paths, SIM_THRESHOLD)

def process_image(image_dir: str, filename: str, reference_img: np.ndarray) -> tuple:
    """å¤„ç†å•å¼ å›¾åƒï¼ˆè®¡ç®—ç¾å­¦åˆ†ã€æ¸…æ™°åº¦ã€ä¸å‚è€ƒå›¾çš„ç›¸ä¼¼åº¦ï¼‰"""
    try:
        img_path = os.path.join(image_dir, filename)
        aes_score = get_aesthetic_score(img_path)
        blur_score = compute_blur_score(img_path)
        sim_score = face_similarity_tools(cv2.imread(img_path), reference_img)
        return (filename, aes_score, blur_score, sim_score)
    except Exception as e:
        print(f"âŒ å¤„ç† {filename} å¤±è´¥: {e}")
        return (filename, -1.0, -1.0, -1.0)

def export_csv(data: list, csv_path: str):
    """å¯¼å‡ºè¯„åˆ†ç»“æœåˆ°CSV"""
    df = pd.DataFrame(data, columns=["æ–‡ä»¶å", "ç¾å­¦åˆ†", "æ¸…æ™°åº¦", "äººè„¸ç›¸ä¼¼åº¦"])
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
